{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMP3314 ML\n",
    "Guo Shunhua 3035447635 \n",
    "<br>\n",
    "# <u>Logistic regression Construction</u>\n",
    "\n",
    "## Overview\n",
    "1. [Introduction](#s1) \n",
    "2. [Decision Tree](#s2)\n",
    "3. [Random Forest](#s3)\n",
    "4. [Load data & Apply the model](#s4) \n",
    "5. [Parameter Analysis](#s5)\n",
    "\n",
    "\n",
    "----- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’s1’></a>\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "This notebook will implement a Multi-class Decision Tree, Random Forest, and then train implemented model to provided data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Import Libraries</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 not detected\")\n",
    "                    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’s2’></a>\n",
    "\n",
    "## 2 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the lecture, the following impurity measures or splitting criteria are commonly used in decision trees:\n",
    "1. Gini impurity ( gini ), \n",
    "2. Entropy ( entropy ), \n",
    "3. Classification error ( classificationError ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.0\n"
     ]
    }
   ],
   "source": [
    "# define gini value for one group (classes)\n",
    "def gini(group, numClass):\n",
    "    gini = 1  \n",
    "    size = len(group)\n",
    "    \n",
    "    if size != 0:\n",
    "        y_v = [y[-1] for y in group]\n",
    "        for c in range(numClass):\n",
    "            #proportion of each class\n",
    "            p = y_v.count(c)/size\n",
    "            gini -= p**2\n",
    "    return gini\n",
    "\n",
    "print(gini([[1, 1], [1, 0]], 2), gini([[1, 0], [1, 0]], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# define entropy value for one group (classes)\n",
    "def entropy(group, numClass):\n",
    "    entropy = 0  \n",
    "    size = len(group)\n",
    "    \n",
    "    if size != 0:\n",
    "        y_v = [y[-1] for y in group]\n",
    "        for c in range(numClass):\n",
    "            #proportion of each class\n",
    "            p = y_v.count(c)/size\n",
    "            if p != 0:\n",
    "                entropy -= p * math.log(p,2)\n",
    "        return entropy\n",
    "    else:\n",
    "        return 1\n",
    "print(entropy([[1, 1], [1, 0]], 2), gini([[1, 0], [1, 0]], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.0\n"
     ]
    }
   ],
   "source": [
    "# define classificationError value for one group (classes)\n",
    "def classificationError(group, numClass):\n",
    "    p = []\n",
    "    size = len(group)\n",
    "    \n",
    "    if size != 0:\n",
    "        y_v = [y[-1] for y in group]\n",
    "        for c in range(numClass):\n",
    "            #proportion of each class\n",
    "            p.append(y_v.count(c)/size)\n",
    "        return 1 - np.max(p)     \n",
    "    else:\n",
    "        return 1\n",
    "print(classificationError([[1, 1], [1, 0]], 2), gini([[1, 0], [1, 0]], 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a split\n",
    "def split(group, x_index, val):\n",
    "    #record group spliting result\n",
    "    left = []\n",
    "    right = []\n",
    "    for x in group:\n",
    "        if x[x_index] < val:\n",
    "            left.append(x)\n",
    "        else:\n",
    "            right.append(x)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a tree structure to store the Decision Tree training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTnode(object):\n",
    "    def __init__(self, level=0):\n",
    "        self.level = level\n",
    "        \n",
    "        self.x_index = None\n",
    "        self.val = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "    def update_info(self, x_index, val, left, right):\n",
    "        self.x_index = x_index\n",
    "        self.val = val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def add_class(self, c):\n",
    "        self.c = c\n",
    "        \n",
    "    def get_info(self):\n",
    "        return self.x_index, self.val, self.left, self.right\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, criterion='gini', max_depth=4, min_size=1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "    \n",
    "    # ------------------- Main ------------------- #\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.numClass = int(np.max(y)) + 1\n",
    "        X_w_y = np.concatenate((X, y), axis = 1)\n",
    "        \n",
    "        # gives a group split with max_depth\n",
    "        # record each node spliting\n",
    "        self.root = DTnode()\n",
    "        self.split_node(self.root, X_w_y, self.max_depth)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    #recursively split the group\n",
    "    def split_node(self, node, group, max_depth):\n",
    "        if len(group) == 0:\n",
    "            return\n",
    "        \n",
    "        if max_depth <= 0 or len(group) <= self.min_size:\n",
    "            self.assign_class(node, group)\n",
    "            return \n",
    "        \n",
    "        if self.select_split(group) != None:\n",
    "            (left, right), x_index, val = self.select_split(group)\n",
    "            leftnode = DTnode(level=node.level+1)\n",
    "            rightnode = DTnode(level=node.level+1)\n",
    "            node.update_info(x_index, val, leftnode, rightnode)\n",
    "\n",
    "            self.split_node(leftnode, left, max_depth-1)\n",
    "            self.split_node(rightnode, right, max_depth-1)\n",
    "        else:\n",
    "            self.assign_class(node, group)\n",
    "            return\n",
    "    \n",
    "    # record class classification at the node\n",
    "    def assign_class(self, node, group):\n",
    "        y_v = [y[-1] for y in group]\n",
    "        c_v = [y_v.count(c) for c in range(self.numClass)]\n",
    "        c = np.argmax(c_v)\n",
    "        node.add_class(c)\n",
    "        return\n",
    "            \n",
    "    # select split\n",
    "    def select_split(self, group):\n",
    "        info_gain_v = []\n",
    "        info_gain_comb = []\n",
    "        for x_index in range(len(group[0])-1):\n",
    "            if len(group) > 100:   \n",
    "                x_v = [x[x_index] for x in group]\n",
    "                xmin, xmax = np.min(x_v), np.max(x_v)\n",
    "                step_size = (xmax - xmin)/20\n",
    "                val = xmin + step_size\n",
    "                while val < xmax:\n",
    "                    ig = self.info_gain(group, x_index, val)\n",
    "                    info_gain_v.append(ig)\n",
    "                    info_gain_comb.append([x_index, val])\n",
    "                    val += step_size\n",
    "\n",
    "            else:\n",
    "                for x in group:\n",
    "                    ig = self.info_gain(group, x_index, x[x_index])\n",
    "                    info_gain_v.append(ig)\n",
    "                    info_gain_comb.append([x_index, x[x_index]])\n",
    "\n",
    "        # get the split resulting maximum infomation gain\n",
    "        if np.max(info_gain_v) == 0:\n",
    "            # information gain == 0, then no need to split\n",
    "            return None\n",
    "        else:\n",
    "            x_index, val = info_gain_comb[np.argmax(info_gain_v)]\n",
    "            return split(group, x_index, val), x_index, val    \n",
    "\n",
    "    # compute information gain at a split point\n",
    "    def info_gain(self, group, x_index, val):\n",
    "        size = len(group)\n",
    "        \n",
    "        #parent's loss\n",
    "        p = self.loss(group)\n",
    "\n",
    "        left, right = split(group, x_index, val)\n",
    "\n",
    "        # proportional left/right group's loss\n",
    "        l = self.loss(left) * len(left) / size\n",
    "        r = self.loss(right) * len(right) / size\n",
    "\n",
    "        return p - l - r\n",
    "    \n",
    "    #define the impurity/error function called loss\n",
    "    def loss(self, group):\n",
    "        if self.criterion == \"gini\":\n",
    "            return gini(group, self.numClass)\n",
    "        elif self.criterion == \"entropy\":\n",
    "            return entropy(group, self.numClass)\n",
    "        elif self.criterion == \"ce\":\n",
    "            return classificationError(group, self.numClass)\n",
    "        else:\n",
    "            raise Exception(\"Sorry, no this impurity/error function implemented.\")\n",
    "\n",
    "    # ------------------- Evalution ------------------- #\n",
    "    \n",
    "    # predict single x input\n",
    "    def predict(self, node, x):\n",
    "        x_index, val, leftnode, rightnode = node.get_info()\n",
    "        \n",
    "        if x_index == None or val == None:\n",
    "            return node.c\n",
    "        elif x[x_index] < val:\n",
    "            return self.predict(leftnode, x)\n",
    "        else:\n",
    "            return self.predict(rightnode, x)\n",
    "    \n",
    "    #compute model accuracy\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = [[self.predict(self.root, x)] for x in X]\n",
    "        return (y_pred == y).sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’s3’></a>\n",
    "\n",
    "\n",
    "## 3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, criterion='gini', max_depth=4, min_size=1, \n",
    "                 k_estimators=25, random_seed=42):\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.k_estimators = k_estimators\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "    # ------------------- Main ------------------- #\n",
    "\n",
    "    def fit(self, X, y, n_sample_size=None, d_features=None):\n",
    "        \n",
    "        # define n sample size if not yet defined\n",
    "        if n_sample_size == None:\n",
    "            print(\"Reminder: n_sample_size is not specified here, assume to be total/5\")\n",
    "            self.n_sample_size = int(len(y)/5)\n",
    "        else:\n",
    "            self.n_sample_size = n_sample_size\n",
    "        \n",
    "        # define d feature number if not yet defined\n",
    "        if d_features == None:\n",
    "            print(\"Reminder: d_features is not specified here, assume to be the whole feature set\")\n",
    "            self.d_features = len(X[0])\n",
    "        else:\n",
    "            self.d_features = d_features\n",
    "        \n",
    "        self.trees = []\n",
    "        for i in range(self.k_estimators):\n",
    "            #set random seed\n",
    "            random.seed(int(self.random_seed))\n",
    "            self.random_seed += 1\n",
    "            \n",
    "            rnd_index = random.sample(range(len(y)), self.n_sample_size)\n",
    "            feature_index = set(random.sample(range(len(X[0])), self.d_features))\n",
    "            \n",
    "            # masking non-seleted columns (set to all zeros)\n",
    "            X_sampled = X[rnd_index]\n",
    "            X_sampled = [X_sampled[:,i] if i in feature_index else X_sampled[:,i]*0 for i in range(len(X[0]))]\n",
    "            X_sampled = np.stack(X_sampled, axis = 1)\n",
    "            \n",
    "            y_sampled = y[rnd_index]\n",
    "            \n",
    "            self.trees.append(self.return_DT(X_sampled, y_sampled))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def return_DT(self, X_sampled, y_sampled):\n",
    "        tree = DecisionTree(criterion=self.criterion, \n",
    "                            max_depth=self.max_depth, min_size=self.min_size)\n",
    "        return tree.fit(X_sampled, y_sampled)\n",
    "    \n",
    "    # ------------------- Evalution ------------------- #\n",
    "    \n",
    "    #predict single x sample\n",
    "    def predict(self, x):\n",
    "        y_preds = [tree.predict(tree.root, x) for tree in self.trees]\n",
    "        return statistics.mode(y_preds)\n",
    "    \n",
    "    # calculate accuracy of the model\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = [[self.predict(x)] for x in X]\n",
    "        return (y_pred == y).sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’s4’></a>\n",
    "\n",
    "## 4 Load data & Apply the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv file, which located in the same folder as this notebook.\n",
    "\n",
    "X_train = pd.read_csv(\"dataset_files/iris_X_train.csv\").to_numpy()\n",
    "y_train = pd.read_csv(\"dataset_files/iris_y_train.csv\").to_numpy()\n",
    "X_test = pd.read_csv(\"dataset_files/iris_X_test.csv\").to_numpy()\n",
    "y_test = pd.read_csv(\"dataset_files/iris_y_test.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Decision Tree training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.98\n",
      "test accuracy:  0.98\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy: \", tree.accuracy(X_train, y_train))\n",
    "print(\"test accuracy: \", tree.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder: n_sample_size is not specified here, assume to be total/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c445a5e14d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3d374b9a089b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_sample_size, d_features)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mrnd_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mfeature_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# masking non-seleted columns (set to all zeros)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "forest = RandomForest(k_estimators=100)\n",
    "forest.fit(X_train, y_train, d_features=5)\n",
    "\n",
    "print(\"training accuracy: \", forest.accuracy(X_train, y_train))\n",
    "print(\"test accuracy: \", forest.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from csv file, which located in the same folder as this notebook.\n",
    "\n",
    "X_train_raw = pd.read_csv(\"dataset_files/car_X_train.csv\").to_numpy()\n",
    "y_train_raw = pd.read_csv(\"dataset_files/car_y_train.csv\").to_numpy()\n",
    "X_test_raw = pd.read_csv(\"dataset_files/car_X_test.csv\").to_numpy()\n",
    "y_test_raw = pd.read_csv(\"dataset_files/car_y_test.csv\").to_numpy()\n",
    "\n",
    "print(\"X input Meaning: \",\n",
    "      \"buying price, price of the maintenance, number of doors, \",\n",
    "      \"number of persons to carry, size of luggage boot, safety of the car\")\n",
    "\n",
    "print(\"X training Class List: \", set(X_train_raw.flatten()))\n",
    "print(\"y training Class List: \", set(y_train_raw.flatten()))\n",
    "print(\"X test Class List: \", set(X_test_raw.flatten()))\n",
    "print(\"y test Class List: \", set(y_test_raw.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform str indicators in data to numeric classes\n",
    "x_dict = {\n",
    "    'vhigh' : 4, \n",
    "    'high' : 3,\n",
    "    'med' : 2,\n",
    "    'low' : 1, \n",
    "     \n",
    "    'small' : 1,\n",
    "    # 'med' : 2,\n",
    "    'big' : 3,\n",
    "    \n",
    "    # '2' : 2, \n",
    "    # '4' : 4,\n",
    "    'more' : 6, \n",
    "    \n",
    "    '2' : 2, \n",
    "    '3' : 3,\n",
    "    '4' : 4,\n",
    "    '5more' : 6,\n",
    "}\n",
    "\n",
    "y_dict = {\n",
    "    'acc':1, 'good':2, 'unacc':0, 'vgood':3\n",
    "}\n",
    "\n",
    "def transform_X(X):\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        X[i] = [x_dict[i] for i in x]\n",
    "    return X\n",
    "\n",
    "def transform_y(y):\n",
    "    return [[y_dict[row[0]]] for row in y]\n",
    "\n",
    "X_train = transform_X(X_train_raw)\n",
    "y_train = np.array(transform_y(y_train_raw))\n",
    "X_test = transform_X(X_test_raw)\n",
    "y_test = np.array(transform_y(y_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Decision Tree training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"training accuracy: \", tree.accuracy(X_train, y_train))\n",
    "print(\"test accuracy: \", tree.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForest(k_estimators=100)\n",
    "forest.fit(X_train, y_train, d_features=5)\n",
    "\n",
    "print(\"training accuracy: \", forest.accuracy(X_train, y_train))\n",
    "print(\"test accuracy: \", forest.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=’s5’></a>\n",
    "\n",
    "## 5 Parameter Analysis\n",
    "\n",
    "The data set used in this section is Car Data Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record accuracy in each case\n",
    "accu = []\n",
    "# Gini\n",
    "tree = DecisionTree()\n",
    "tree.fit(X_train, y_train)\n",
    "forest = RandomForest()\n",
    "forest.fit(X_train, y_train, n_sample_size=100, d_features=5)\n",
    "\n",
    "accu.append([tree.accuracy(X_train, y_train), tree.accuracy(X_test, y_test),\n",
    "            forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "# Entropy\n",
    "tree = DecisionTree(criterion=\"entropy\")\n",
    "tree.fit(X_train, y_train)\n",
    "forest = RandomForest(criterion=\"entropy\")\n",
    "forest.fit(X_train, y_train, n_sample_size=100, d_features=5)\n",
    "\n",
    "accu.append([tree.accuracy(X_train, y_train), tree.accuracy(X_test, y_test),\n",
    "            forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "# ClassificationError\n",
    "tree = DecisionTree(criterion=\"ce\")\n",
    "tree.fit(X_train, y_train)\n",
    "forest = RandomForest(criterion=\"ce\")\n",
    "forest.fit(X_train, y_train, n_sample_size=100, d_features=5)\n",
    "\n",
    "accu.append([tree.accuracy(X_train, y_train), tree.accuracy(X_test, y_test),\n",
    "            forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "accu_pd = pd.DataFrame(accu, \n",
    "             columns=['DT training', 'DT test', 'RF training', 'RF test'],\n",
    "             index = [\"Gini\", \"Entropy\", \"ClassificationError\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(accu_pd, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,20)\n",
    "\n",
    "# record accuracy in each case\n",
    "accu = []\n",
    "\n",
    "for i in x:\n",
    "    tree = DecisionTree(max_depth=i)\n",
    "    tree.fit(X_train, y_train)\n",
    "    forest = RandomForest(max_depth=i)\n",
    "    forest.fit(X_train, y_train, n_sample_size=100, d_features=5)\n",
    "\n",
    "    accu.append([tree.accuracy(X_train, y_train), tree.accuracy(X_test, y_test),\n",
    "                forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "accu = np.array(accu).T\n",
    "\n",
    "plt.plot(x, accu[0], label=\"DT training\")\n",
    "plt.plot(x, accu[1], label=\"DT test\")\n",
    "plt.plot(x, accu[2], label=\"RF training\")\n",
    "plt.plot(x, accu[3], label=\"RF test\")\n",
    "\n",
    "plt.xlabel(\"max_dept\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tree estimators in Random Forest (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(10,200,5)\n",
    "\n",
    "# record accuracy in each case\n",
    "accu = []\n",
    "\n",
    "for i in x:\n",
    "    forest = RandomForest(k_estimators=i)\n",
    "    forest.fit(X_train, y_train, n_sample_size=100, d_features=5)\n",
    "\n",
    "    accu.append([forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "accu = np.array(accu).T\n",
    "\n",
    "plt.plot(x, accu[0], label=\"training\")\n",
    "plt.plot(x, accu[1], label=\"test\")\n",
    "\n",
    "plt.xlabel(\"Number of tree estimators in Random Forest\")\n",
    "plt.ylabel(\"RT Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of sample size in each sample bagging (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(int(len(y_train)/20),len(y_train)+1,int(len(y_train)/20))\n",
    "\n",
    "# record accuracy in each case\n",
    "accu = []\n",
    "\n",
    "for i in x:\n",
    "    forest = RandomForest()\n",
    "    forest.fit(X_train, y_train, n_sample_size=i, d_features=5)\n",
    "    #print(forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test))\n",
    "    accu.append([forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "accu = np.array(accu).T\n",
    "\n",
    "plt.plot(x, accu[0], label=\"training\")\n",
    "plt.plot(x, accu[1], label=\"test\")\n",
    "\n",
    "plt.xlabel(\"Number of tree estimators in Random Forest\")\n",
    "plt.ylabel(\"RT Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of feature selected in each sample bagging (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, len(X_train[0]) + 1)\n",
    "\n",
    "# record accuracy in each case\n",
    "accu = []\n",
    "\n",
    "for i in x:\n",
    "    forest = RandomForest()\n",
    "    forest.fit(X_train, y_train, n_sample_size=100, d_features=i)\n",
    "    #print(forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test))\n",
    "    accu.append([forest.accuracy(X_train, y_train), forest.accuracy(X_test, y_test)])\n",
    "\n",
    "accu = np.array(accu).T\n",
    "\n",
    "plt.plot(x, accu[0], label=\"training\")\n",
    "plt.plot(x, accu[1], label=\"test\")\n",
    "\n",
    "plt.xlabel(\"Number of tree estimators in Random Forest\")\n",
    "plt.ylabel(\"RT Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
